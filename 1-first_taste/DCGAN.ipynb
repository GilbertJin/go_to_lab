{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DCGAN for Generating Anime Faces"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prerequisites"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\r\n",
    "import torch.nn as nn\r\n",
    "import torchvision.transforms as transforms\r\n",
    "\r\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\r\n",
    "batch_size = 128"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==> Preparing data..\n"
     ]
    }
   ],
   "source": [
    "print('==> Preparing data..')\r\n",
    "import torchvision.datasets as datasets\r\n",
    "train_transforms = transforms.Compose([\r\n",
    "    #transforms.RandomRotation(10),\r\n",
    "    #transforms.RandomResizedCrop(224),\r\n",
    "    #transforms.RandomHorizontalFlip(),\r\n",
    "    transforms.ToTensor(),\r\n",
    "    transforms.Normalize((.5, .5, .5), (.5, .5, .5))])\r\n",
    "train_dir = 'konachan/anime'\r\n",
    "train_datasets = datasets.ImageFolder(train_dir, transform=train_transforms)\r\n",
    "train_loader = torch.utils.data.DataLoader(train_datasets, batch_size=batch_size, shuffle=True, num_workers=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adopted Module: DCGAN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generator & Discriminator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==> Building model..\n"
     ]
    }
   ],
   "source": [
    "print('==> Building model..')\r\n",
    "\r\n",
    "class Generator(nn.Module):\r\n",
    "    def __init__(self, channels):\r\n",
    "        super().__init__()\r\n",
    "        # Filters [1024, 512, 256]\r\n",
    "        # Input_dim = 100\r\n",
    "        # Output_dim = C (number of channels)\r\n",
    "        self.main_module = nn.Sequential(\r\n",
    "            # Z latent vector 100\r\n",
    "            nn.ConvTranspose2d(in_channels=100, out_channels=1024, kernel_size=4, stride=1, padding=0),\r\n",
    "            nn.BatchNorm2d(num_features=1024),\r\n",
    "            nn.ReLU(True),\r\n",
    "\r\n",
    "            # State (1024x4x4)\r\n",
    "            nn.ConvTranspose2d(in_channels=1024, out_channels=512, kernel_size=4, stride=2, padding=1),\r\n",
    "            nn.BatchNorm2d(num_features=512),\r\n",
    "            nn.ReLU(True),\r\n",
    "\r\n",
    "            # State (512x8x8)\r\n",
    "            nn.ConvTranspose2d(in_channels=512, out_channels=256, kernel_size=4, stride=2, padding=1),\r\n",
    "            nn.BatchNorm2d(num_features=256),\r\n",
    "            nn.ReLU(True),\r\n",
    "\r\n",
    "            # State (256x16x16)\r\n",
    "            #nn.ConvTranspose2d(in_channels=256, out_channels=channels, kernel_size=4, stride=2, padding=1)\r\n",
    "            nn.ConvTranspose2d(in_channels=256, out_channels=128, kernel_size=4, stride=2, padding=1),\r\n",
    "            nn.BatchNorm2d(num_features=128),\r\n",
    "            nn.ReLU(True),\r\n",
    "\r\n",
    "            # State (128x32x32)\r\n",
    "            nn.ConvTranspose2d(in_channels=128, out_channels=channels, kernel_size=5, stride=3, padding=1)\r\n",
    "        )\r\n",
    "            # output of main module --> Image (Cx96x96)\r\n",
    "        self.output = nn.Tanh()\r\n",
    "\r\n",
    "    def forward(self, x):\r\n",
    "        x = self.main_module(x)\r\n",
    "        return self.output(x)\r\n",
    "\r\n",
    "\r\n",
    "class Discriminator(nn.Module):\r\n",
    "    def __init__(self, channels):\r\n",
    "        super().__init__()\r\n",
    "        # Filters [256, 512, 1024] ?\r\n",
    "        # Input_dim = channels (Cx64x64) ?\r\n",
    "        # Output_dim = 1\r\n",
    "        self.main_module = nn.Sequential(\r\n",
    "            # Image (Cx96x96)\r\n",
    "            #nn.Conv2d(in_channels=channels, out_channels=256, kernel_size=4, stride=2, padding=1),\r\n",
    "            nn.Conv2d(in_channels=channels, out_channels=128, kernel_size=5, stride=3, padding=1),\r\n",
    "            nn.LeakyReLU(0.2, inplace=True),\r\n",
    "\r\n",
    "            # State (128x32x32)\r\n",
    "            nn.Conv2d(in_channels=128, out_channels=256, kernel_size=4, stride=2, padding=1),\r\n",
    "            nn.BatchNorm2d(256),\r\n",
    "            nn.LeakyReLU(0.2, inplace=True),\r\n",
    "\r\n",
    "            # State (256x16x16)\r\n",
    "            nn.Conv2d(in_channels=256, out_channels=512, kernel_size=4, stride=2, padding=1),\r\n",
    "            nn.BatchNorm2d(512),\r\n",
    "            nn.LeakyReLU(0.2, inplace=True),\r\n",
    "\r\n",
    "            # State (512x8x8)\r\n",
    "            nn.Conv2d(in_channels=512, out_channels=1024, kernel_size=4, stride=2, padding=1),\r\n",
    "            nn.BatchNorm2d(1024),\r\n",
    "            nn.LeakyReLU(0.2, inplace=True))\r\n",
    "            # outptut of main module --> State (1024x4x4)\r\n",
    "\r\n",
    "        self.output = nn.Sequential(\r\n",
    "            nn.Conv2d(in_channels=1024, out_channels=1, kernel_size=4, stride=1, padding=0),\r\n",
    "            # Output 1\r\n",
    "            nn.Sigmoid())\r\n",
    "\r\n",
    "    def forward(self, x):\r\n",
    "        x = self.main_module(x)\r\n",
    "        return self.output(x)\r\n",
    "\r\n",
    "    def feature_extraction(self, x):\r\n",
    "        # Use discriminator for feature extraction then flatten to vector of 16384 features\r\n",
    "        x = self.main_module(x)\r\n",
    "        return x.view(-1, 1024 * 4 * 4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Network & Utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.autograd import Variable\r\n",
    "from deprecated_utils.tensorboard_logger.tensorboard_logger import Logger\r\n",
    "import time as t\r\n",
    "import os\r\n",
    "from torchvision import utils\r\n",
    "\r\n",
    "epochs = 20\r\n",
    "arg_cuda = True\r\n",
    "channels = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DCGAN_MODEL(object):\r\n",
    "    def __init__(self):\r\n",
    "        self.G = Generator(channels)\r\n",
    "        self.D = Discriminator(channels)\r\n",
    "        self.C = channels\r\n",
    "\r\n",
    "        # binary cross entropy loss and optimizer\r\n",
    "        self.loss = nn.BCELoss()\r\n",
    "\r\n",
    "        self.cuda = False\r\n",
    "        self.cuda_index = 0\r\n",
    "        # check if cuda is available\r\n",
    "        self.check_cuda(arg_cuda)\r\n",
    "\r\n",
    "        # Using lower learning rate than suggested by (ADAM authors) lr=0.0002  and Beta_1 = 0.5 instead od 0.9 works better [Radford2015]\r\n",
    "        self.d_optimizer = torch.optim.Adam(self.D.parameters(), lr=0.0002, betas=(0.5, 0.999))\r\n",
    "        self.g_optimizer = torch.optim.Adam(self.G.parameters(), lr=0.0002, betas=(0.5, 0.999))\r\n",
    "\r\n",
    "        self.epochs = epochs\r\n",
    "        self.batch_size = batch_size\r\n",
    "\r\n",
    "        # Set the logger\r\n",
    "        self.logger = Logger('./logs')\r\n",
    "        self.number_of_images = 10\r\n",
    "        print('==> DCGAN model Initialzation..')\r\n",
    "\r\n",
    "    # cuda support\r\n",
    "    def check_cuda(self, cuda_flag=False):\r\n",
    "        if cuda_flag:\r\n",
    "            print('==> Check if CUDA supported..')\r\n",
    "            self.cuda = True\r\n",
    "            self.D.cuda(self.cuda_index)\r\n",
    "            self.G.cuda(self.cuda_index)\r\n",
    "            self.loss = nn.BCELoss().cuda(self.cuda_index)\r\n",
    "            print(self.cuda)\r\n",
    "\r\n",
    "\r\n",
    "    def train(self, train_loader):\r\n",
    "        self.t_begin = t.time()\r\n",
    "        generator_iter = 0\r\n",
    "        #self.file = open(\"inception_score_graph.txt\", \"w\")\r\n",
    "\r\n",
    "        for epoch in range(self.epochs):\r\n",
    "            self.epoch_start_time = t.time()\r\n",
    "\r\n",
    "            for i, (images, _) in enumerate(train_loader):\r\n",
    "                # Check if round number of batches\r\n",
    "                if i == train_loader.dataset.__len__() // self.batch_size:\r\n",
    "                    break\r\n",
    "\r\n",
    "                z = torch.rand((self.batch_size, 100, 1, 1))\r\n",
    "                real_labels = torch.ones(self.batch_size)\r\n",
    "                fake_labels = torch.zeros(self.batch_size)\r\n",
    "\r\n",
    "                if self.cuda:\r\n",
    "                    images, z = Variable(images).cuda(self.cuda_index), Variable(z).cuda(self.cuda_index)\r\n",
    "                    real_labels, fake_labels = Variable(real_labels).cuda(self.cuda_index), Variable(fake_labels).cuda(self.cuda_index)\r\n",
    "                else:\r\n",
    "                    images, z = Variable(images), Variable(z)\r\n",
    "                    real_labels, fake_labels = Variable(real_labels), Variable(fake_labels)\r\n",
    "\r\n",
    "\r\n",
    "                # Train discriminator\r\n",
    "                # Compute BCE_Loss using real images\r\n",
    "                outputs = self.D(images)\r\n",
    "                outputs = outputs[:, 0, 0, :]\r\n",
    "                outputs = outputs.squeeze(-1)\r\n",
    "                d_loss_real = self.loss(outputs, real_labels)\r\n",
    "                real_score = outputs\r\n",
    "\r\n",
    "                # Compute BCE Loss using fake images\r\n",
    "                if self.cuda:\r\n",
    "                    z = Variable(torch.randn(self.batch_size, 100, 1, 1)).cuda(self.cuda_index)\r\n",
    "                else:\r\n",
    "                    z = Variable(torch.randn(self.batch_size, 100, 1, 1))\r\n",
    "                fake_images = self.G(z)\r\n",
    "                outputs = self.D(fake_images)\r\n",
    "                outputs = outputs[:, 0, 0, :]\r\n",
    "                outputs = outputs.squeeze(-1)\r\n",
    "                d_loss_fake = self.loss(outputs, fake_labels)\r\n",
    "                fake_score = outputs\r\n",
    "\r\n",
    "                # Optimize discriminator\r\n",
    "                d_loss = d_loss_real + d_loss_fake\r\n",
    "                self.D.zero_grad()\r\n",
    "                d_loss.backward()\r\n",
    "                self.d_optimizer.step()\r\n",
    "\r\n",
    "                # Train generator\r\n",
    "                # Compute loss with fake images\r\n",
    "                if self.cuda:\r\n",
    "                    z = Variable(torch.randn(self.batch_size, 100, 1, 1)).cuda(self.cuda_index)\r\n",
    "                else:\r\n",
    "                    z = Variable(torch.randn(self.batch_size, 100, 1, 1))\r\n",
    "                fake_images = self.G(z)\r\n",
    "                outputs = self.D(fake_images)\r\n",
    "                outputs = outputs[:, 0, 0, :]\r\n",
    "                outputs = outputs.squeeze(-1)\r\n",
    "                g_loss = self.loss(outputs, real_labels)\r\n",
    "\r\n",
    "                # Optimize generator\r\n",
    "                self.D.zero_grad()\r\n",
    "                self.G.zero_grad()\r\n",
    "                g_loss.backward()\r\n",
    "                self.g_optimizer.step()\r\n",
    "                generator_iter += 1\r\n",
    "\r\n",
    "\r\n",
    "                if generator_iter % 1000 == 0:\r\n",
    "                    # Workaround because graphic card memory can't store more than 800+ examples in memory for generating image\r\n",
    "                    # Therefore doing loop and generating 800 examples and stacking into list of samples to get 8000 generated images\r\n",
    "                    # This way Inception score is more correct since there are different generated examples from every class of Inception model\r\n",
    "                    # sample_list = []\r\n",
    "                    # for i in range(10):\r\n",
    "                    #     z = Variable(torch.randn(800, 100, 1, 1)).cuda(self.cuda_index)\r\n",
    "                    #     samples = self.G(z)\r\n",
    "                    #     sample_list.append(samples.data.cpu().numpy())\r\n",
    "                    #\r\n",
    "                    # # Flattening list of lists into one list of numpy arrays\r\n",
    "                    # new_sample_list = list(chain.from_iterable(sample_list))\r\n",
    "                    # print(\"Calculating Inception Score over 8k generated images\")\r\n",
    "                    # # Feeding list of numpy arrays\r\n",
    "                    # inception_score = get_inception_score(new_sample_list, cuda=True, batch_size=32,\r\n",
    "                    #                                       resize=True, splits=10)\r\n",
    "                    print('==> Epoch-{}'.format(epoch + 1))\r\n",
    "                    self.save_model()\r\n",
    "\r\n",
    "                    if not os.path.exists('training_result_images/'):\r\n",
    "                        os.makedirs('training_result_images/')\r\n",
    "\r\n",
    "                    # Denormalize images and save them in grid 8x8\r\n",
    "                    z = Variable(torch.randn(800, 100, 1, 1)).cuda(self.cuda_index)\r\n",
    "                    samples = self.G(z)\r\n",
    "                    samples = samples.mul(0.5).add(0.5)\r\n",
    "                    samples = samples.data.cpu()[:64]\r\n",
    "                    grid = utils.make_grid(samples)\r\n",
    "                    utils.save_image(grid, 'training_result_images/img_generatori_iter_{}.png'.format(str(generator_iter).zfill(3)))\r\n",
    "\r\n",
    "                    time = t.time() - self.t_begin\r\n",
    "                    #print(\"Inception score: {}\".format(inception_score))\r\n",
    "                    print(\"Generator iter: {}\".format(generator_iter))\r\n",
    "                    print(\"Time {}\".format(time))\r\n",
    "\r\n",
    "                    # Write to file inception_score, gen_iters, time\r\n",
    "                    #output = str(generator_iter) + \" \" + str(time) + \" \" + str(inception_score[0]) + \"\\n\"\r\n",
    "                    #self.file.write(output)\r\n",
    "\r\n",
    "\r\n",
    "                if ((i + 1) % 100) == 0:\r\n",
    "                    print(\"Epoch: [%2d] [%4d/%4d] D_loss: %.8f, G_loss: %.8f\" %\r\n",
    "                          ((epoch + 1), (i + 1), train_loader.dataset.__len__() // self.batch_size, d_loss.data, g_loss.data))\r\n",
    "\r\n",
    "                    z = Variable(torch.randn(self.batch_size, 100, 1, 1).cuda(self.cuda_index))\r\n",
    "\r\n",
    "                    # TensorBoard logging\r\n",
    "                    # Log the scalar values\r\n",
    "                    info = {\r\n",
    "                        'd_loss': d_loss.data.cpu().item(),\r\n",
    "                        'g_loss': g_loss.data.cpu().item()\r\n",
    "                    }\r\n",
    "\r\n",
    "                    for tag, value in info.items():\r\n",
    "                        self.logger._scalar_summary(tag, value, generator_iter)\r\n",
    "\r\n",
    "                    # Log values and gradients of the parameters\r\n",
    "                    for tag, value in self.D.named_parameters():\r\n",
    "                        tag = tag.replace('.', '/')\r\n",
    "                        self.logger._histogram_summary(tag, self.to_np(value), generator_iter)\r\n",
    "                        self.logger._histogram_summary(tag + '/grad', self.to_np(value.grad), generator_iter)\r\n",
    "\r\n",
    "                    # Log the images while training\r\n",
    "                    info = {\r\n",
    "                        'real_images': self.real_images(images, self.number_of_images),\r\n",
    "                        'generated_images': self.generate_img(z, self.number_of_images)\r\n",
    "                    }\r\n",
    "\r\n",
    "                    for tag, images in info.items():\r\n",
    "                        self.logger._image_summary(tag, images, generator_iter)\r\n",
    "\r\n",
    "\r\n",
    "        self.t_end = t.time()\r\n",
    "        print('Time of training-{}'.format((self.t_end - self.t_begin)))\r\n",
    "        #self.file.close()\r\n",
    "\r\n",
    "        # Save the trained parameters\r\n",
    "        self.save_model()\r\n",
    "\r\n",
    "    def evaluate(self, test_loader, D_model_path, G_model_path):\r\n",
    "        self.load_model(D_model_path, G_model_path)\r\n",
    "        z = Variable(torch.randn(self.batch_size, 100, 1, 1)).cuda(self.cuda_index)\r\n",
    "        samples = self.G(z)\r\n",
    "        samples = samples.mul(0.5).add(0.5)\r\n",
    "        samples = samples.data.cpu()\r\n",
    "        grid = utils.make_grid(samples)\r\n",
    "        print(\"Grid of 8x8 images saved to 'dgan_model_image.png'.\")\r\n",
    "        utils.save_image(grid, 'dgan_model_image.png')\r\n",
    "\r\n",
    "    def real_images(self, images, number_of_images):\r\n",
    "        if (self.C == 3):\r\n",
    "            return self.to_np(images.view(-1, self.C, 96, 96)[:self.number_of_images])\r\n",
    "        else:\r\n",
    "            return self.to_np(images.view(-1, 96, 96)[:self.number_of_images])\r\n",
    "\r\n",
    "    def generate_img(self, z, number_of_images):\r\n",
    "        samples = self.G(z).data.cpu().numpy()[:number_of_images]\r\n",
    "        generated_images = []\r\n",
    "        for sample in samples:\r\n",
    "            if self.C == 3:\r\n",
    "                generated_images.append(sample.reshape(self.C, 96, 96))\r\n",
    "            else:\r\n",
    "                generated_images.append(sample.reshape(96, 96))\r\n",
    "        return generated_images\r\n",
    "\r\n",
    "    def to_np(self, x):\r\n",
    "        return x.data.cpu().numpy()\r\n",
    "\r\n",
    "    def save_model(self):\r\n",
    "        torch.save(self.G.state_dict(), './generator.pkl')\r\n",
    "        torch.save(self.D.state_dict(), './discriminator.pkl')\r\n",
    "        print('Models save to ./generator.pkl & ./discriminator.pkl ')\r\n",
    "\r\n",
    "    def load_model(self, D_model_filename, G_model_filename):\r\n",
    "        D_model_path = os.path.join(os.getcwd(), D_model_filename)\r\n",
    "        G_model_path = os.path.join(os.getcwd(), G_model_filename)\r\n",
    "        self.D.load_state_dict(torch.load(D_model_path))\r\n",
    "        self.G.load_state_dict(torch.load(G_model_path))\r\n",
    "        print('Generator model loaded from {}.'.format(G_model_path))\r\n",
    "        print('Discriminator model loaded from {}-'.format(D_model_path))\r\n",
    "\r\n",
    "    def generate_latent_walk(self, number):\r\n",
    "        if not os.path.exists('interpolated_images/'):\r\n",
    "            os.makedirs('interpolated_images/')\r\n",
    "\r\n",
    "        # Interpolate between twe noise(z1, z2) with number_int steps between\r\n",
    "        number_int = 10\r\n",
    "        z_intp = torch.FloatTensor(1, 100, 1, 1)\r\n",
    "        z1 = torch.randn(1, 100, 1, 1)\r\n",
    "        z2 = torch.randn(1, 100, 1, 1)\r\n",
    "        if self.cuda:\r\n",
    "            z_intp = z_intp.cuda()\r\n",
    "            z1 = z1.cuda()\r\n",
    "            z2 = z2.cuda()\r\n",
    "\r\n",
    "        z_intp = Variable(z_intp)\r\n",
    "        images = []\r\n",
    "        alpha = 1.0 / float(number_int + 1)\r\n",
    "        print(alpha)\r\n",
    "        for i in range(1, number_int + 1):\r\n",
    "            z_intp.data = z1*alpha + z2*(1.0 - alpha)\r\n",
    "            alpha += alpha\r\n",
    "            fake_im = self.G(z_intp)\r\n",
    "            fake_im = fake_im.mul(0.5).add(0.5) #denormalize\r\n",
    "            images.append(fake_im.view(self.C, 96, 96).data.cpu())\r\n",
    "\r\n",
    "        grid = utils.make_grid(images, nrow=number_int)\r\n",
    "        utils.save_image(grid, 'interpolated_images/interpolated_{}.png'.format(str(number).zfill(3)))\r\n",
    "        print(\"Saved interpolated images to interpolated_images/interpolated_{}.\".format(str(number).zfill(3)))\r\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DCGAN model initalization.\n",
      "Cuda enabled flag: \n",
      "True\n",
      "Epoch: [ 1] [ 100/ 537] D_loss: 0.88756299, G_loss: 3.42816544\n",
      "Epoch: [ 1] [ 200/ 537] D_loss: 1.29067290, G_loss: 3.77012134\n",
      "Epoch: [ 1] [ 300/ 537] D_loss: 1.03245938, G_loss: 2.78110361\n",
      "Epoch: [ 1] [ 400/ 537] D_loss: 1.06614709, G_loss: 3.23071241\n",
      "Epoch: [ 1] [ 500/ 537] D_loss: 0.98437411, G_loss: 7.58571720\n",
      "Epoch: [ 2] [ 100/ 537] D_loss: 0.49723065, G_loss: 4.67902184\n",
      "Epoch: [ 2] [ 200/ 537] D_loss: 0.66492969, G_loss: 6.36657238\n",
      "Epoch: [ 2] [ 300/ 537] D_loss: 0.89227748, G_loss: 5.67321920\n",
      "Epoch: [ 2] [ 400/ 537] D_loss: 0.43032280, G_loss: 4.61956835\n",
      "Epoch-2\n",
      "Models save to ./generator.pkl & ./discriminator.pkl \n",
      "Generator iter: 1000\n",
      "Time 1641.2801699638367\n",
      "Epoch: [ 2] [ 500/ 537] D_loss: 0.33173230, G_loss: 5.30700636\n",
      "Epoch: [ 3] [ 100/ 537] D_loss: 0.23408586, G_loss: 5.66482496\n",
      "Epoch: [ 3] [ 200/ 537] D_loss: 0.55358577, G_loss: 6.33971214\n",
      "Epoch: [ 3] [ 300/ 537] D_loss: 0.25391385, G_loss: 4.80063820\n",
      "Epoch: [ 3] [ 400/ 537] D_loss: 0.89184964, G_loss: 6.46725035\n",
      "Epoch: [ 3] [ 500/ 537] D_loss: 0.40762681, G_loss: 4.64874506\n",
      "Epoch: [ 4] [ 100/ 537] D_loss: 0.62676674, G_loss: 11.38050461\n",
      "Epoch: [ 4] [ 200/ 537] D_loss: 2.69528079, G_loss: 7.39241123\n",
      "Epoch: [ 4] [ 300/ 537] D_loss: 0.32438946, G_loss: 7.25591230\n",
      "Epoch-4\n",
      "Models save to ./generator.pkl & ./discriminator.pkl \n",
      "Generator iter: 2000\n",
      "Time 3283.726802110672\n",
      "Epoch: [ 4] [ 400/ 537] D_loss: 0.18086991, G_loss: 4.63278627\n",
      "Epoch: [ 4] [ 500/ 537] D_loss: 0.33560169, G_loss: 4.65545750\n",
      "Epoch: [ 5] [ 100/ 537] D_loss: 0.09972695, G_loss: 5.43011522\n",
      "Epoch: [ 5] [ 200/ 537] D_loss: 0.77859199, G_loss: 7.01931715\n",
      "Epoch: [ 5] [ 300/ 537] D_loss: 0.39196232, G_loss: 5.78040171\n",
      "Epoch: [ 5] [ 400/ 537] D_loss: 0.19017228, G_loss: 4.69909859\n",
      "Epoch: [ 5] [ 500/ 537] D_loss: 0.03957091, G_loss: 2.99668550\n",
      "Epoch: [ 6] [ 100/ 537] D_loss: 0.21844235, G_loss: 6.86226845\n",
      "Epoch: [ 6] [ 200/ 537] D_loss: 0.08551661, G_loss: 4.17768288\n",
      "Epoch: [ 6] [ 300/ 537] D_loss: 0.06714720, G_loss: 5.57726002\n",
      "Epoch-6\n",
      "Models save to ./generator.pkl & ./discriminator.pkl \n",
      "Generator iter: 3000\n",
      "Time 4924.665491580963\n",
      "Epoch: [ 6] [ 400/ 537] D_loss: 0.13610421, G_loss: 7.81256008\n",
      "Epoch: [ 6] [ 500/ 537] D_loss: 0.12018638, G_loss: 5.20073700\n",
      "Epoch: [ 7] [ 100/ 537] D_loss: 0.10086518, G_loss: 3.52676678\n",
      "Epoch: [ 7] [ 200/ 537] D_loss: 0.44263628, G_loss: 8.57376480\n",
      "Epoch: [ 7] [ 300/ 537] D_loss: 1.63583672, G_loss: 11.78342247\n",
      "Epoch: [ 7] [ 400/ 537] D_loss: 0.12739086, G_loss: 5.26772308\n",
      "Epoch: [ 7] [ 500/ 537] D_loss: 0.04840008, G_loss: 5.91110945\n",
      "Epoch: [ 8] [ 100/ 537] D_loss: 0.02241226, G_loss: 4.94013214\n",
      "Epoch: [ 8] [ 200/ 537] D_loss: 0.03172981, G_loss: 10.89618874\n",
      "Epoch-8\n",
      "Models save to ./generator.pkl & ./discriminator.pkl \n",
      "Generator iter: 4000\n",
      "Time 6565.496160030365\n",
      "Epoch: [ 8] [ 300/ 537] D_loss: 0.08849358, G_loss: 5.35767555\n",
      "Epoch: [ 8] [ 400/ 537] D_loss: 0.04718800, G_loss: 6.81400585\n",
      "Epoch: [ 8] [ 500/ 537] D_loss: 0.06774064, G_loss: 5.70291805\n",
      "Epoch: [ 9] [ 100/ 537] D_loss: 0.02679032, G_loss: 6.46078157\n",
      "Epoch: [ 9] [ 200/ 537] D_loss: 0.26286882, G_loss: 8.29920769\n",
      "Epoch: [ 9] [ 300/ 537] D_loss: 0.06212913, G_loss: 5.11933517\n",
      "Epoch: [ 9] [ 400/ 537] D_loss: 0.03166927, G_loss: 8.66370106\n",
      "Epoch: [ 9] [ 500/ 537] D_loss: 0.27199554, G_loss: 9.49921417\n",
      "Epoch: [10] [ 100/ 537] D_loss: 0.22688742, G_loss: 7.39105082\n",
      "Epoch-10\n",
      "Models save to ./generator.pkl & ./discriminator.pkl \n",
      "Generator iter: 5000\n",
      "Time 8206.32245016098\n",
      "Epoch: [10] [ 200/ 537] D_loss: 0.05502826, G_loss: 5.82699394\n",
      "Epoch: [10] [ 300/ 537] D_loss: 0.04754548, G_loss: 10.20718575\n",
      "Epoch: [10] [ 400/ 537] D_loss: 0.11628386, G_loss: 6.82454395\n",
      "Epoch: [10] [ 500/ 537] D_loss: 0.35242048, G_loss: 10.58277893\n",
      "Epoch: [11] [ 100/ 537] D_loss: 0.11040418, G_loss: 6.33024025\n",
      "Epoch: [11] [ 200/ 537] D_loss: 0.10079509, G_loss: 7.59173346\n",
      "Epoch: [11] [ 300/ 537] D_loss: 0.11622823, G_loss: 5.83292389\n",
      "Epoch: [11] [ 400/ 537] D_loss: 0.11232841, G_loss: 8.72968292\n",
      "Epoch: [11] [ 500/ 537] D_loss: 0.01230638, G_loss: 7.18488121\n",
      "Epoch-12\n",
      "Models save to ./generator.pkl & ./discriminator.pkl \n",
      "Generator iter: 6000\n",
      "Time 9846.947891950607\n",
      "Epoch: [12] [ 100/ 537] D_loss: 0.10469334, G_loss: 6.09008312\n",
      "Epoch: [12] [ 200/ 537] D_loss: 0.05594056, G_loss: 5.89317989\n",
      "Epoch: [12] [ 300/ 537] D_loss: 0.11154113, G_loss: 5.47365952\n",
      "Epoch: [12] [ 400/ 537] D_loss: 0.02306378, G_loss: 5.67472267\n",
      "Epoch: [12] [ 500/ 537] D_loss: 0.25766176, G_loss: 11.58922863\n",
      "Epoch: [13] [ 100/ 537] D_loss: 0.04170761, G_loss: 6.49587393\n",
      "Epoch: [13] [ 200/ 537] D_loss: 0.08257431, G_loss: 5.85009956\n",
      "Epoch: [13] [ 300/ 537] D_loss: 0.11383087, G_loss: 5.73193026\n",
      "Epoch: [13] [ 400/ 537] D_loss: 0.08597969, G_loss: 6.49383450\n",
      "Epoch: [13] [ 500/ 537] D_loss: 0.26769543, G_loss: 8.44476223\n",
      "Epoch-14\n",
      "Models save to ./generator.pkl & ./discriminator.pkl \n",
      "Generator iter: 7000\n",
      "Time 11487.71502327919\n",
      "Epoch: [14] [ 100/ 537] D_loss: 0.18063082, G_loss: 5.80824327\n",
      "Epoch: [14] [ 200/ 537] D_loss: 0.05012845, G_loss: 14.81690693\n",
      "Epoch: [14] [ 300/ 537] D_loss: 0.21970135, G_loss: 6.97774696\n",
      "Epoch: [14] [ 400/ 537] D_loss: 0.10747253, G_loss: 7.38332367\n",
      "Epoch: [14] [ 500/ 537] D_loss: 0.06045604, G_loss: 5.19917393\n",
      "Epoch: [15] [ 100/ 537] D_loss: 0.12710860, G_loss: 7.71674538\n",
      "Epoch: [15] [ 200/ 537] D_loss: 0.06188444, G_loss: 5.79439974\n",
      "Epoch: [15] [ 300/ 537] D_loss: 0.00411985, G_loss: 8.39433384\n",
      "Epoch: [15] [ 400/ 537] D_loss: 0.24363022, G_loss: 5.38109636\n",
      "Epoch-15\n",
      "Models save to ./generator.pkl & ./discriminator.pkl \n",
      "Generator iter: 8000\n",
      "Time 13131.160254478455\n",
      "Epoch: [15] [ 500/ 537] D_loss: 0.17062588, G_loss: 14.06334114\n",
      "Epoch: [16] [ 100/ 537] D_loss: 0.05475895, G_loss: 4.74765778\n",
      "Epoch: [16] [ 200/ 537] D_loss: 0.04947883, G_loss: 4.95270491\n",
      "Epoch: [16] [ 300/ 537] D_loss: 0.39829105, G_loss: 17.46767807\n",
      "Epoch: [16] [ 400/ 537] D_loss: 0.08350442, G_loss: 4.98001003\n",
      "Epoch: [16] [ 500/ 537] D_loss: 0.02234080, G_loss: 8.59601402\n",
      "Epoch: [17] [ 100/ 537] D_loss: 0.04634712, G_loss: 11.12449646\n",
      "Epoch: [17] [ 200/ 537] D_loss: 0.05784498, G_loss: 6.82141733\n",
      "Epoch: [17] [ 300/ 537] D_loss: 0.04602832, G_loss: 6.26769257\n",
      "Epoch: [17] [ 400/ 537] D_loss: 0.20912650, G_loss: 11.63980865\n",
      "Epoch-17\n",
      "Models save to ./generator.pkl & ./discriminator.pkl \n",
      "Generator iter: 9000\n",
      "Time 14779.82294178009\n",
      "Epoch: [17] [ 500/ 537] D_loss: 0.52157044, G_loss: 8.84837723\n",
      "Epoch: [18] [ 100/ 537] D_loss: 0.05320615, G_loss: 7.44467497\n",
      "Epoch: [18] [ 200/ 537] D_loss: 0.11469898, G_loss: 4.27919865\n",
      "Epoch: [18] [ 300/ 537] D_loss: 0.18137313, G_loss: 6.78599834\n",
      "Epoch: [18] [ 400/ 537] D_loss: 0.06885764, G_loss: 7.69740868\n",
      "Epoch: [18] [ 500/ 537] D_loss: 0.12774615, G_loss: 8.19443035\n",
      "Epoch: [19] [ 100/ 537] D_loss: 0.79971427, G_loss: 8.25633430\n",
      "Epoch: [19] [ 200/ 537] D_loss: 0.05878234, G_loss: 3.16111946\n",
      "Epoch: [19] [ 300/ 537] D_loss: 6.94978905, G_loss: 15.24025059\n",
      "Epoch-19\n",
      "Models save to ./generator.pkl & ./discriminator.pkl \n",
      "Generator iter: 10000\n",
      "Time 16432.07425904274\n",
      "Epoch: [19] [ 400/ 537] D_loss: 0.19863844, G_loss: 10.66670418\n",
      "Epoch: [19] [ 500/ 537] D_loss: 0.05671270, G_loss: 6.95666981\n",
      "Epoch: [20] [ 100/ 537] D_loss: 0.11270414, G_loss: 8.34281158\n",
      "Epoch: [20] [ 200/ 537] D_loss: 0.02451405, G_loss: 6.53697300\n",
      "Epoch: [20] [ 300/ 537] D_loss: 0.06441782, G_loss: 6.57287598\n",
      "Epoch: [20] [ 400/ 537] D_loss: 0.06231357, G_loss: 5.84891462\n",
      "Epoch: [20] [ 500/ 537] D_loss: 0.09323033, G_loss: 8.20971203\n",
      "Time of training-17651.711441755295\n",
      "Models save to ./generator.pkl & ./discriminator.pkl \n"
     ]
    }
   ],
   "source": [
    "model = DCGAN_MODEL()\n",
    "model.train(train_loader)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.1 64-bit",
   "metadata": {
    "interpreter": {
     "hash": "2c5162f4330e5a08843439af36f076d864bbc634ce564c8238d7f8c62bd127df"
    }
   },
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  },
  "orig_nbformat": 2
 },
 "nbformat": 4,
 "nbformat_minor": 2
}