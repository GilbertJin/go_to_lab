{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.9.1 64-bit",
   "metadata": {
    "interpreter": {
     "hash": "2c5162f4330e5a08843439af36f076d864bbc634ce564c8238d7f8c62bd127df"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Train CIFAR10 with PyTorch.'''\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import torch.backends.cudnn as cudnn\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "#import argparse\n",
    "#parser = argparse.ArgumentParser(description='PyTorch CIFAR10 Training')\n",
    "#parser.add_argument('--lr', default = 0.1, type=float, help='learning rate')\n",
    "#parser.add_argument('--resume', '-r', action='store_true',\n",
    "#                   help='resume from checkpoint')\n",
    "#args = parser.parse_args()\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "best_acc = 0  # best test accuracy\n",
    "start_epoch = 0  # start from epoch 0 or last checkpoint epoch\n",
    "resume = True # if resume from the saved file\n",
    "learning_rate = 0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "==> Preparing data..\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "# Data\n",
    "print('==> Preparing data..')\n",
    "transform_train = transforms.Compose([\n",
    "    transforms.RandomCrop(32, padding = 4),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
    "])\n",
    "\n",
    "transform_test = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
    "])\n",
    "\n",
    "trainset = torchvision.datasets.CIFAR10(root = './cifar10', train = True, download = True, transform = transform_train)\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size = 200, shuffle = True, num_workers = 2)\n",
    "\n",
    "testset = torchvision.datasets.CIFAR10(root = './cifar10', train = False, download = True, transform = transform_test)\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size = 200, shuffle = False, num_workers = 2)\n",
    "\n",
    "classes = ('plane', 'car', 'bird', 'cat', 'deer',\n",
    "           'dog', 'frog', 'horse', 'ship', 'truck')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "==> Building model..\n",
      "==> Resuming from checkpoint..\n"
     ]
    }
   ],
   "source": [
    "# Model\n",
    "print('==> Building model..')\n",
    "'''DLA in PyTorch.\n",
    "\n",
    "Reference:\n",
    "    Deep Layer Aggregation. https://arxiv.org/abs/1707.06484\n",
    "'''\n",
    "class BasicBlock(nn.Module):\n",
    "    expansion = 1\n",
    "\n",
    "    def __init__(self, in_planes, planes, stride = 1):\n",
    "        super(BasicBlock, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_planes, planes, kernel_size = 3, stride = stride, padding = 1, bias = False)\n",
    "        self.bn1 = nn.BatchNorm2d(planes)\n",
    "        self.conv2 = nn.Conv2d(planes, planes, kernel_size = 3, stride = 1, padding = 1, bias = False)\n",
    "        self.bn2 = nn.BatchNorm2d(planes)\n",
    "\n",
    "        self.shortcut = nn.Sequential()\n",
    "        if stride != 1 or in_planes != self.expansion * planes:\n",
    "            self.shortcut = nn.Sequential(\n",
    "                nn.Conv2d(in_planes, self.expansion * planes, kernel_size = 1, stride = stride, bias = False),\n",
    "                nn.BatchNorm2d(self.expansion * planes)\n",
    "            )\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = F.relu(self.bn1(self.conv1(x)))\n",
    "        out = self.bn2(self.conv2(out))\n",
    "        out += self.shortcut(x)\n",
    "        out = F.relu(out)\n",
    "        return out\n",
    "\n",
    "\n",
    "class Root(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, kernel_size = 1):\n",
    "        super(Root, self).__init__()\n",
    "        self.conv = nn.Conv2d(in_channels, out_channels, kernel_size, stride = 1, padding = (kernel_size - 1) // 2, bias = False)\n",
    "        self.bn = nn.BatchNorm2d(out_channels)\n",
    "\n",
    "    def forward(self, xs):\n",
    "        x = torch.cat(xs, 1)\n",
    "        out = F.relu(self.bn(self.conv(x)))\n",
    "        return out\n",
    "\n",
    "\n",
    "class Tree(nn.Module):\n",
    "    def __init__(self, block, in_channels, out_channels, level = 1, stride = 1):\n",
    "        super(Tree, self).__init__()\n",
    "        self.level = level\n",
    "        if level == 1:\n",
    "            self.root = Root(2 * out_channels, out_channels)\n",
    "            self.left_node = block(in_channels, out_channels, stride = stride)\n",
    "            self.right_node = block(out_channels, out_channels, stride = 1)\n",
    "        else:\n",
    "            self.root = Root((level + 2) * out_channels, out_channels)\n",
    "            for i in reversed(range(1, level)):\n",
    "                subtree = Tree(block, in_channels, out_channels, level = i, stride = stride)\n",
    "                self.__setattr__('level_%d' % i, subtree)\n",
    "            self.prev_root = block(in_channels, out_channels, stride = stride)\n",
    "            self.left_node = block(out_channels, out_channels, stride = 1)\n",
    "            self.right_node = block(out_channels, out_channels, stride = 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        xs = [self.prev_root(x)] if self.level > 1 else []\n",
    "        for i in reversed(range(1, self.level)):\n",
    "            level_i = self.__getattr__('level_%d' % i)\n",
    "            x = level_i(x)\n",
    "            xs.append(x)\n",
    "        x = self.left_node(x)\n",
    "        xs.append(x)\n",
    "        x = self.right_node(x)\n",
    "        xs.append(x)\n",
    "        out = self.root(xs)\n",
    "        return out\n",
    "\n",
    "\n",
    "class DLA(nn.Module):\n",
    "    def __init__(self, block = BasicBlock, num_classes = 10):\n",
    "        super(DLA, self).__init__()\n",
    "        self.base = nn.Sequential(\n",
    "            nn.Conv2d(3, 16, kernel_size = 3, stride = 1, padding = 1, bias = False),\n",
    "            nn.BatchNorm2d(16),\n",
    "            nn.ReLU(True)\n",
    "        )\n",
    "\n",
    "        self.layer1 = nn.Sequential(\n",
    "            nn.Conv2d(16, 16, kernel_size = 3, stride = 1, padding = 1, bias = False),\n",
    "            nn.BatchNorm2d(16),\n",
    "            nn.ReLU(True)\n",
    "        )\n",
    "\n",
    "        self.layer2 = nn.Sequential(\n",
    "            nn.Conv2d(16, 32, kernel_size = 3, stride = 1, padding = 1, bias = False),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.ReLU(True)\n",
    "        )\n",
    "\n",
    "        self.layer3 = Tree(block,  32,  64, level = 1, stride = 1)\n",
    "        self.layer4 = Tree(block,  64, 128, level = 2, stride = 2)\n",
    "        self.layer5 = Tree(block, 128, 256, level = 2, stride = 2)\n",
    "        self.layer6 = Tree(block, 256, 512, level = 1, stride = 2)\n",
    "        self.linear = nn.Linear(512, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.base(x)\n",
    "        out = self.layer1(out)\n",
    "        out = self.layer2(out)\n",
    "        out = self.layer3(out)\n",
    "        out = self.layer4(out)\n",
    "        out = self.layer5(out)\n",
    "        out = self.layer6(out)\n",
    "        out = F.avg_pool2d(out, 4)\n",
    "        out = out.view(out.size(0), -1)\n",
    "        out = self.linear(out)\n",
    "        return out\n",
    "# net = VGG('VGG19')\n",
    "# net = ResNet18()\n",
    "# net = PreActResNet18()\n",
    "# net = GoogLeNet()\n",
    "# net = DenseNet121()\n",
    "# net = ResNeXt29_2x64d()\n",
    "# net = MobileNet()\n",
    "# net = MobileNetV2()\n",
    "# net = DPN92()\n",
    "# net = ShuffleNetG2()\n",
    "# net = SENet18()\n",
    "# net = ShuffleNetV2(1)\n",
    "# net = EfficientNetB0()\n",
    "# net = RegNetX_200MF()\n",
    "# net = SimpleDLA()\n",
    "net = DLA()\n",
    "net = net.to(device)\n",
    "if device == 'cuda':\n",
    "    net = torch.nn.DataParallel(net)\n",
    "    cudnn.benchmark = True\n",
    "\n",
    "if resume: #args.resume:\n",
    "    # Load checkpoint.\n",
    "    print('==> Resuming from checkpoint..')\n",
    "    assert os.path.isdir('cifar10/checkpoint'), 'Error: no checkpoint directory found!'\n",
    "    checkpoint = torch.load('./cifar10/checkpoint/ckpt.pth')\n",
    "    net.load_state_dict(checkpoint['net'])\n",
    "    best_acc = checkpoint['acc']\n",
    "    start_epoch = checkpoint['epoch']\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(net.parameters(), lr = learning_rate,\n",
    "                      momentum = 0.9, weight_decay = 5e-4)\n",
    "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max = 200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\n",
      "Epoch: 34\n",
      "49 250 Loss: 0.032 | Acc: 99.030% (9903/10000)\n",
      "99 250 Loss: 0.037 | Acc: 98.785% (19757/20000)\n",
      "149 250 Loss: 0.039 | Acc: 98.690% (29607/30000)\n",
      "199 250 Loss: 0.040 | Acc: 98.647% (39459/40000)\n",
      "249 250 Loss: 0.041 | Acc: 98.624% (49312/50000)\n",
      "49 50 Loss: 0.245 | Acc: 93.190% (9319/10000)\n",
      "\n",
      "Epoch: 35\n",
      "49 250 Loss: 0.040 | Acc: 98.580% (9858/10000)\n",
      "99 250 Loss: 0.039 | Acc: 98.630% (19726/20000)\n",
      "149 250 Loss: 0.039 | Acc: 98.667% (29600/30000)\n",
      "199 250 Loss: 0.039 | Acc: 98.680% (39472/40000)\n",
      "249 250 Loss: 0.039 | Acc: 98.690% (49345/50000)\n",
      "49 50 Loss: 0.240 | Acc: 93.230% (9323/10000)\n",
      "\n",
      "Epoch: 36\n",
      "49 250 Loss: 0.036 | Acc: 98.770% (9877/10000)\n",
      "99 250 Loss: 0.040 | Acc: 98.630% (19726/20000)\n",
      "149 250 Loss: 0.040 | Acc: 98.650% (29595/30000)\n",
      "199 250 Loss: 0.040 | Acc: 98.698% (39479/40000)\n",
      "249 250 Loss: 0.041 | Acc: 98.642% (49321/50000)\n",
      "49 50 Loss: 0.260 | Acc: 92.670% (9267/10000)\n",
      "\n",
      "Epoch: 37\n",
      "49 250 Loss: 0.033 | Acc: 98.970% (9897/10000)\n",
      "99 250 Loss: 0.034 | Acc: 98.905% (19781/20000)\n",
      "149 250 Loss: 0.034 | Acc: 98.867% (29660/30000)\n",
      "199 250 Loss: 0.036 | Acc: 98.750% (39500/40000)\n",
      "249 250 Loss: 0.037 | Acc: 98.742% (49371/50000)\n",
      "49 50 Loss: 0.255 | Acc: 92.990% (9299/10000)\n",
      "\n",
      "Epoch: 38\n",
      "49 250 Loss: 0.034 | Acc: 98.880% (9888/10000)\n",
      "99 250 Loss: 0.034 | Acc: 98.870% (19774/20000)\n",
      "149 250 Loss: 0.035 | Acc: 98.827% (29648/30000)\n",
      "199 250 Loss: 0.036 | Acc: 98.815% (39526/40000)\n",
      "249 250 Loss: 0.036 | Acc: 98.792% (49396/50000)\n",
      "49 50 Loss: 0.262 | Acc: 92.790% (9279/10000)\n",
      "\n",
      "Epoch: 39\n",
      "49 250 Loss: 0.034 | Acc: 98.890% (9889/10000)\n",
      "99 250 Loss: 0.033 | Acc: 98.975% (19795/20000)\n",
      "149 250 Loss: 0.033 | Acc: 98.970% (29691/30000)\n",
      "199 250 Loss: 0.034 | Acc: 98.912% (39565/40000)\n",
      "249 250 Loss: 0.034 | Acc: 98.888% (49444/50000)\n",
      "49 50 Loss: 0.259 | Acc: 93.280% (9328/10000)\n",
      "\n",
      "Epoch: 40\n",
      "49 250 Loss: 0.028 | Acc: 99.150% (9915/10000)\n",
      "99 250 Loss: 0.028 | Acc: 99.165% (19833/20000)\n",
      "149 250 Loss: 0.030 | Acc: 99.063% (29719/30000)\n",
      "199 250 Loss: 0.031 | Acc: 98.998% (39599/40000)\n",
      "249 250 Loss: 0.031 | Acc: 98.970% (49485/50000)\n",
      "49 50 Loss: 0.279 | Acc: 92.710% (9271/10000)\n",
      "\n",
      "Epoch: 41\n",
      "49 250 Loss: 0.028 | Acc: 99.100% (9910/10000)\n",
      "99 250 Loss: 0.029 | Acc: 99.045% (19809/20000)\n",
      "149 250 Loss: 0.031 | Acc: 98.957% (29687/30000)\n",
      "199 250 Loss: 0.031 | Acc: 98.987% (39595/40000)\n",
      "249 250 Loss: 0.032 | Acc: 98.958% (49479/50000)\n",
      "49 50 Loss: 0.271 | Acc: 92.910% (9291/10000)\n",
      "\n",
      "Epoch: 42\n",
      "49 250 Loss: 0.033 | Acc: 98.880% (9888/10000)\n",
      "99 250 Loss: 0.031 | Acc: 98.960% (19792/20000)\n",
      "149 250 Loss: 0.032 | Acc: 98.933% (29680/30000)\n",
      "199 250 Loss: 0.034 | Acc: 98.888% (39555/40000)\n",
      "249 250 Loss: 0.033 | Acc: 98.888% (49444/50000)\n",
      "49 50 Loss: 0.263 | Acc: 93.030% (9303/10000)\n",
      "\n",
      "Epoch: 43\n",
      "49 250 Loss: 0.032 | Acc: 98.920% (9892/10000)\n",
      "99 250 Loss: 0.030 | Acc: 99.075% (19815/20000)\n",
      "149 250 Loss: 0.032 | Acc: 98.993% (29698/30000)\n",
      "199 250 Loss: 0.032 | Acc: 98.990% (39596/40000)\n",
      "249 250 Loss: 0.032 | Acc: 98.968% (49484/50000)\n",
      "49 50 Loss: 0.276 | Acc: 92.990% (9299/10000)\n",
      "\n",
      "Epoch: 44\n",
      "49 250 Loss: 0.030 | Acc: 99.050% (9905/10000)\n",
      "99 250 Loss: 0.031 | Acc: 98.985% (19797/20000)\n",
      "149 250 Loss: 0.030 | Acc: 98.973% (29692/30000)\n",
      "199 250 Loss: 0.033 | Acc: 98.907% (39563/40000)\n",
      "249 250 Loss: 0.032 | Acc: 98.958% (49479/50000)\n",
      "49 50 Loss: 0.257 | Acc: 92.820% (9282/10000)\n",
      "\n",
      "Epoch: 45\n",
      "49 250 Loss: 0.028 | Acc: 99.120% (9912/10000)\n",
      "99 250 Loss: 0.027 | Acc: 99.135% (19827/20000)\n",
      "149 250 Loss: 0.027 | Acc: 99.147% (29744/30000)\n",
      "199 250 Loss: 0.029 | Acc: 99.088% (39635/40000)\n",
      "249 250 Loss: 0.030 | Acc: 99.022% (49511/50000)\n",
      "49 50 Loss: 0.275 | Acc: 92.820% (9282/10000)\n",
      "\n",
      "Epoch: 46\n",
      "49 250 Loss: 0.029 | Acc: 99.050% (9905/10000)\n",
      "99 250 Loss: 0.031 | Acc: 98.945% (19789/20000)\n",
      "149 250 Loss: 0.032 | Acc: 98.913% (29674/30000)\n",
      "199 250 Loss: 0.032 | Acc: 98.935% (39574/40000)\n",
      "249 250 Loss: 0.032 | Acc: 98.946% (49473/50000)\n",
      "49 50 Loss: 0.262 | Acc: 92.980% (9298/10000)\n",
      "\n",
      "Epoch: 47\n",
      "49 250 Loss: 0.026 | Acc: 99.170% (9917/10000)\n",
      "99 250 Loss: 0.026 | Acc: 99.150% (19830/20000)\n",
      "149 250 Loss: 0.026 | Acc: 99.173% (29752/30000)\n",
      "199 250 Loss: 0.027 | Acc: 99.135% (39654/40000)\n",
      "249 250 Loss: 0.027 | Acc: 99.118% (49559/50000)\n",
      "49 50 Loss: 0.305 | Acc: 92.160% (9216/10000)\n",
      "\n",
      "Epoch: 48\n",
      "49 250 Loss: 0.031 | Acc: 98.980% (9898/10000)\n",
      "99 250 Loss: 0.029 | Acc: 99.075% (19815/20000)\n",
      "149 250 Loss: 0.029 | Acc: 99.070% (29721/30000)\n",
      "199 250 Loss: 0.027 | Acc: 99.103% (39641/40000)\n",
      "249 250 Loss: 0.029 | Acc: 99.050% (49525/50000)\n",
      "49 50 Loss: 0.279 | Acc: 92.860% (9286/10000)\n",
      "\n",
      "Epoch: 49\n",
      "49 250 Loss: 0.031 | Acc: 99.040% (9904/10000)\n",
      "99 250 Loss: 0.029 | Acc: 99.075% (19815/20000)\n",
      "149 250 Loss: 0.031 | Acc: 98.993% (29698/30000)\n",
      "199 250 Loss: 0.031 | Acc: 98.983% (39593/40000)\n",
      "249 250 Loss: 0.032 | Acc: 98.948% (49474/50000)\n",
      "49 50 Loss: 0.273 | Acc: 92.710% (9271/10000)\n",
      "\n",
      "Epoch: 50\n",
      "49 250 Loss: 0.029 | Acc: 99.050% (9905/10000)\n",
      "99 250 Loss: 0.028 | Acc: 99.110% (19822/20000)\n",
      "149 250 Loss: 0.025 | Acc: 99.200% (29760/30000)\n",
      "199 250 Loss: 0.025 | Acc: 99.183% (39673/40000)\n",
      "249 250 Loss: 0.026 | Acc: 99.130% (49565/50000)\n",
      "49 50 Loss: 0.302 | Acc: 92.180% (9218/10000)\n",
      "\n",
      "Epoch: 51\n",
      "49 250 Loss: 0.027 | Acc: 99.150% (9915/10000)\n",
      "99 250 Loss: 0.027 | Acc: 99.105% (19821/20000)\n",
      "149 250 Loss: 0.027 | Acc: 99.100% (29730/30000)\n",
      "199 250 Loss: 0.026 | Acc: 99.123% (39649/40000)\n",
      "249 250 Loss: 0.027 | Acc: 99.082% (49541/50000)\n",
      "49 50 Loss: 0.278 | Acc: 92.790% (9279/10000)\n",
      "\n",
      "Epoch: 52\n",
      "49 250 Loss: 0.028 | Acc: 99.050% (9905/10000)\n",
      "99 250 Loss: 0.029 | Acc: 99.045% (19809/20000)\n",
      "149 250 Loss: 0.028 | Acc: 99.090% (29727/30000)\n",
      "199 250 Loss: 0.028 | Acc: 99.073% (39629/40000)\n",
      "249 250 Loss: 0.028 | Acc: 99.080% (49540/50000)\n",
      "49 50 Loss: 0.277 | Acc: 92.790% (9279/10000)\n",
      "\n",
      "Epoch: 53\n",
      "49 250 Loss: 0.026 | Acc: 99.120% (9912/10000)\n",
      "99 250 Loss: 0.028 | Acc: 99.000% (19800/20000)\n",
      "149 250 Loss: 0.030 | Acc: 98.997% (29699/30000)\n",
      "199 250 Loss: 0.030 | Acc: 98.993% (39597/40000)\n",
      "249 250 Loss: 0.031 | Acc: 98.966% (49483/50000)\n",
      "49 50 Loss: 0.285 | Acc: 92.690% (9269/10000)\n"
     ]
    }
   ],
   "source": [
    "# Training\n",
    "def train(epoch):\n",
    "    print('\\nEpoch: %d' % epoch)\n",
    "    net.train()\n",
    "    train_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for batch_idx, (inputs, targets) in enumerate(trainloader):\n",
    "        inputs, targets = inputs.to(device), targets.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = net(inputs)\n",
    "        loss = criterion(outputs, targets)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        train_loss += loss.item()\n",
    "        _, predicted = outputs.max(1)\n",
    "        total += targets.size(0)\n",
    "        correct += predicted.eq(targets).sum().item()\n",
    "\n",
    "        if (batch_idx + 1) % 50 == 0:\n",
    "            print(batch_idx, len(trainloader), 'Loss: %.3f | Acc: %.3f%% (%d/%d)' % (train_loss/(batch_idx+1), 100.*correct/total, correct, total))\n",
    "\n",
    "\n",
    "def test(epoch):\n",
    "    global best_acc\n",
    "    net.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for batch_idx, (inputs, targets) in enumerate(testloader):\n",
    "            inputs, targets = inputs.to(device), targets.to(device)\n",
    "            outputs = net(inputs)\n",
    "            loss = criterion(outputs, targets)\n",
    "\n",
    "            test_loss += loss.item()\n",
    "            _, predicted = outputs.max(1)\n",
    "            total += targets.size(0)\n",
    "            correct += predicted.eq(targets).sum().item()\n",
    "\n",
    "            if (batch_idx + 1) % 50 == 0:\n",
    "                print(batch_idx, len(testloader), 'Loss: %.3f | Acc: %.3f%% (%d/%d)' % (test_loss / (batch_idx + 1), 100. * correct / total, correct, total))\n",
    "\n",
    "    # Save checkpoint.\n",
    "    acc = 100. * correct / total\n",
    "    if acc > best_acc:\n",
    "        print('Saving..')\n",
    "        state = {\n",
    "            'net': net.state_dict(),\n",
    "            'acc': acc,\n",
    "            'epoch': epoch,\n",
    "        }\n",
    "        if not os.path.isdir('cifar10/checkpoint'):\n",
    "            os.mkdir('cifar10/checkpoint')\n",
    "        torch.save(state, './cifar10/checkpoint/ckpt.pth')\n",
    "        best_acc = acc\n",
    "\n",
    "\n",
    "for epoch in range(start_epoch, start_epoch + 20):\n",
    "    train(epoch)\n",
    "    test(epoch)\n",
    "    scheduler.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Accuracy of plane : 90.9 %\nAccuracy of   car : 100.0 %\nAccuracy of  bird : 94.7 %\nAccuracy of   cat : 69.6 %\nAccuracy of  deer : 88.9 %\nAccuracy of   dog : 93.3 %\nAccuracy of  frog : 81.8 %\nAccuracy of horse : 100.0 %\nAccuracy of  ship : 96.0 %\nAccuracy of truck : 100.0 %\nCurrently, the best total accuracy is 93.6%\n"
     ]
    }
   ],
   "source": [
    "class_correct = list(0. for i in range(10))\n",
    "class_total = list(0. for i in range(10))\n",
    "with torch.no_grad():\n",
    "    for data in testloader:\n",
    "        images, labels = data\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        outputs = net(images)\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        c = (predicted == labels).squeeze()\n",
    "        for i in range(4):\n",
    "            label = labels[i]\n",
    "            class_correct[label] += c[i].item()\n",
    "            class_total[label] += 1\n",
    "\n",
    "\n",
    "for i in range(10):\n",
    "    print('Accuracy of %5s : %2.1f %%' % (\n",
    "        classes[i], 100 * class_correct[i] / class_total[i]))\n",
    "\n",
    "print('Currently, the best total accuracy is %2.1f%%' % best_acc)"
   ]
  }
 ]
}